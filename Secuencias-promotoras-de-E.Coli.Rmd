---
title: 'KNN-Algoritm applied to Promoter sequences of E. Coli'
author: "Sheila Fern√°ndez Cisneros"
date: '`r format(Sys.Date(),"%e de %B, %Y")`'
output:
  html_document:
    toc: TRUE
    toc_depth: 2
    theme: cosmo
    toc_float: TRUE
    number_section: FALSE
  pdf_document:
    toc: TRUE
    toc_depth: 2
bibliography: biblio.bib
csl: apa.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r libraries, include=FALSE}
# Libraries 

library(knitr)
library(stringr)
library(class)
library(dplyr)
library(gmodels)
library(ggseqlogo)
library(ggplot2)
library(ROCR)
library(pROC)
library(caret)
```

```{r input, include=FALSE}
# Input / Output variables

file <- "promoters.txt"
file1 <- "promoters_onehot.txt"
```

## 1. Write a section in the report titled 'k-NN Algorithm,' providing a brief explanation of its operation and characteristics. Additionally, include a table outlining its strengths and weaknesses.

# k-NN Algorithm

## Brief explanation of its operation and characteristics:

In general, an algorithm can be defined as a set of well-defined, unambiguous, ordered, and finite rules or instructions designed to solve a specific problem, perform calculations, process data, or complete a task.

To execute an algorithm, data availability is required, often in large quantities. Additionally, the computational power needed may vary depending on the algorithm. Therefore, there is a relationship between these three elements: data availability, computational power, and the algorithm itself.

For each algorithm, the following structure will be presented: a brief introduction to the algorithm, details and hyperparameters, and its strengths and weaknesses.

The k-NN algorithm was initially developed in the 1950s as a non-parametric method in statistics. It can be used for both classification and regression problems.

The prediction is based on the k nearest training samples to the sample being predicted.

-In classification problems, the result is the most frequent class among its k nearest neighbors. 

-In regression problems, the prediction is the average value of the k nearest neighbors. 

The choice of k, representing the number of nearest neighbors, can influence the prediction outcome. It is the only hyperparameter of this algorithm, which needs to be adjusted during the training process. Typically, an odd value is chosen for k to avoid ties in prediction decisions. 

Another important factor is the distance metric used to measure proximity, which depends on the characteristics of the predictor variables. There are various types of distances, with the most commonly used being Euclidean distance. Another example is the Manhattan or City block distance, which is an absolute value.

### Delving into a more detailed decision.

1. Euclidean distance in KNN algorithm

In the K-nearest neighbors (KNN) algorithm, Euclidean distance is a commonly used metric to measure the similarity or dissimilarity between two data points in a feature space. It is named after the Greek mathematician Euclid and is based on the straight-line distance between two points in a Cartesian coordinate system.

The Euclidean distance between two points is calculated using the Pythagorean theorem. It represents the length of the shortest path connecting the two points, which is a straight line. Mathematically, the Euclidean distance between two points (p1, p2) in a d-dimensional feature space can be expressed as:

euclidean_distance = sqrt((p1[1] - p2[1])^2 + (p1[2] - p2[2])^2 + ... + (p1[d] - p2[d])^2)

Where p1[i] and p2[i] represent the values of the ith feature for the two points.

In the context of the KNN algorithm, the Euclidean distance is used to measure the proximity between the query point and the training points. The KNN algorithm finds the k nearest neighbors to the query point based on the Euclidean distance and makes predictions based on the class or average value of the target variable of those nearest neighbors.

The Euclidean distance is commonly used in KNN because it considers the magnitude and direction of the differences between feature values, and it works well for continuous numerical features. However, it may not be the most suitable choice when dealing with data that has categorical or ordinal features, as it assumes that all features contribute equally to the overall distance. In such cases, other distance metrics, such as the city block distance, may be more appropriate.

2. In the K-nearest neighbors (KNN) algorithm, the city block distance, also known as the Manhattan distance, is a metric used to measure the similarity or dissimilarity between two data points in a feature space. It is named after the distance one would need to travel in a city block grid to reach from one point to another, moving only vertically and horizontally.

The city block distance between two points is calculated by summing the absolute differences of their corresponding feature values. It is defined as the sum of the absolute differences between the coordinates of the two points along each dimension. Mathematically, the city block distance between two points (p1, p2) in a d-dimensional feature space can be expressed as:

city_block_distance = |p1[1] - p2[1]| + |p1[2] - p2[2]| + ... + |p1[d] - p2[d]|

Where p1[i] and p2[i] represent the values of the ith feature for the two points.

In the context of the KNN algorithm, the city block distance is used to measure the proximity between the query point and the training points. The KNN algorithm finds the k nearest neighbors to the query point based on the city block distance and makes predictions based on the class or average value of the target variable of those nearest neighbors.

Using the city block distance in the KNN algorithm allows for the consideration of the differences between feature values along each dimension and can be particularly useful when dealing with data that has categorical or ordinal features.

## Strengths and weaknesses of the K-NN algorithm:

| Strengths | Weaknesses |
| -- | -- |
| Simple | Does not produce a model |
| Effective | High memory consumption |
| Does not assume any data distribution | Requires preprocessing for handling missing values |

The K-NN algorithm does not produce a model because it calculates distances between the point to be predicted and all the training data for each prediction. If there are missing values in the data, preprocessing with imputation is required.

## 2. Develop a function in R (or Python) that implements "one-hot" encoding of sequences. Present a simple example of its usage.

Simple example of using one-hot:

The nitrogenous bases that form nucleotides are 4: a, g, c, and t.

We create a vector with them, which we will call baseN:

```{r}
baseN <- c("a","g","c","t")
```

### We create the one-hot function:

```{r}
one.hot<-function(secuencia, bases){
y<-unlist(strsplit(secuencia,"")) # We separate the nitrogenous bases that make up the sequence.
sapply(y,function(x){match(bases,x,nomatch=0)}) # The sapply() function in the R Language takes a list, vector, or data frame as input and gives output in the form of an array or matrix object.
}
```

### Here's an example:

```{r}
sec_ejemplo <- "tactag"
ej <- t(one.hot(sec_ejemplo, bases = baseN))
ej
```

As we can see, we have encoded our example sequence using 0s and 1s based on the initially created vector *baseN*, indicated with numbers 1, 2, 3, and 4, representing the components of baseN. We use zeros to indicate the absence of those components in our example sequence and ones to indicate the presence of each component in our example sequence. 

vector <- t(matriz)
dim(vector) <-prod(dim(vector))
vector

```{r}
# We convert it into a vector:
vector <- t(ej)
dim(vector) <- prod(dim(vector))
vector
```

## 3. Develop a script in R (or Python) that implements a KNN classifier. The script should perform the following sections:

## (a) Read the data from the file promoters.txt and indicate the number of observations per class.

To read the data from the file, let's first display the file in which we have the sequences.

```{r, echo=FALSE}
data <- read.table(file, sep=",", stringsAsFactors = FALSE, header = FALSE) 
head(data)
str(data)
```

We observe that we have `r nrow(data)` observations with `r ncol(data)` variables, all three of which are of character type.

The attributes of the data file are:
1. A symbol {+/-}, indicating the class ("+" = promoter).
2. The name of the promoter sequence. Instances corresponding to non-promoters are labeled by genomic position.
3. The remaining positions correspond to the sequence.

Therefore, the first step we need to take is to convert the first column into a factor with two levels. This will allow us to use the data for further analysis.

```{r}
data$V1 <- factor(data$V1, levels = c("+", "-"))
str(data)
```

To determine the number of sequences for each class "+","-": 

```{r}
data.frame(table(data[,1]))
```

We can check that we have `r nrow(data[data$V1 == "+", ])` promoter sequences and `r nrow(data[data$V1 == "-", ])` non-promoter sequences.

## (b) Transforming nucleotide sequences into numerical vectors using the transformation function developed earlier. If the one-hot encoding function has not been implemented, the transformed data can be accessed by loading the promoters_onehot.txt file.

### Transformation of one-hot encoded data:

The third column of the dataset corresponds to the sequences. Therefore, we apply the one-hot encoding to the third column of the dataset.

```{r}
data_onehot <- apply(as.data.frame(data[,3]), 1, one.hot, baseN)
data_onehot <- t(data_onehot)
data_onehot <- as.data.frame(data_onehot)
#head(data_onehot)
#str(data_onehot)
```

We observe that we have `r nrow(data_onehot)` observations with `r ncol(data_onehot)` variables.

### Dataset Transformation:

We can start by removing the third and second columns from our data to keep only the column that is of interest to us, which is the promoter/non-promoter column (+/-).

```{r}
data_V1 <- select(data, -V3, -V2)
str(data_V1) # We verify that V1 has already been transformed into a factor.
```

We apply a label to the levels of the factor:

```{r}
data_V1$V1 <- factor(data_V1$V1, labels = c("plus","minus"))
head(data_V1)
str(data_V1)
```

We combine the datasets that contain the first column and the third column after one-hot encoding using cbind():

```{r}
dataframe <- cbind (data_V1, data_onehot)
#head(dataframe)
```

We assign a name to the first column:

```{r}
names(dataframe)[1] = "class"
#head(dataframe)
```

## (c) Using the random seed 123, split the data into two parts, one for training (67%) and one for testing (33%).

We proceed to set the specified seed and split the data as required.

```{r}
set.seed(123) # We set the seed
train <- sample(1:nrow(dataframe),round(2*nrow(dataframe)/3,0)) # use the sample function
training <- dataframe[train,] # create the training data
test <- dataframe[-train,] # create the test data

# We create the train and test sets only for the "label" column:
train_label <- data[train,1] 
test_label <- data[-train,1]
```

We show how many promoters there are in the training and test sets:

```{r}
table(training$class)
table(test$class)
```

Another way to do it could be:

```{r}
set.seed(123)
split <- sort(sample(nrow(dataframe), nrow(dataframe)*0.67)) # We select the 67% of the dataset

train1 <- dataframe[split, -1]
test1 <- dataframe[-split, -1]

train1_label <- dataframe[split, 1]
test1_label <- dataframe[-split, 1]
```

## (d) Apply k-NN (k = 1, 5, 11, 21, 51, 71) based on the training set to predict whether the sequences in the test set are promoter or non-promoter sequences. Additionally, create an ROC curve for each k and display the AUC value.

### KNN() (k = 1, 5, 11, 21, 51, 71):

```{r}
test <- test[-1] # we remove the class column
training <- training[-1] # we remove the class column 
```


```{r}
set.seed(123) # we set the seed
pred1 <-knn(train = training, test = test, cl = train_label, k = 1, prob = TRUE)
pred1
CrossTable(x = test_label, y = pred1, prop.chisq = FALSE )
```

```{r}
set.seed(123) # we set the seed
pred5 <-knn(train = training, test = test, cl = train_label, k = 5, prob = TRUE)
pred5
CrossTable(x = test_label, y = pred5, prop.chisq = FALSE )
```

```{r}
set.seed(123) # we set the seed
pred11 <-knn(train = training, test = test, cl = train_label, k = 11, prob = TRUE)
pred11
CrossTable(x = test_label, y = pred11, prop.chisq = FALSE )
```

```{r}
set.seed(123) # we set the seed
pred21 <-knn(train = training, test = test, cl = train_label, k = 21, prob = TRUE)
pred21
CrossTable(x = test_label, y = pred21, prop.chisq = FALSE )
```

```{r}
set.seed(123) # we set the seed
pred51 <-knn(train = training, test = test, cl = train_label, k = 51, prob = TRUE)
pred51
CrossTable(x = test_label, y = pred51, prop.chisq = FALSE )
```

```{r}
set.seed(123) # we set the seed
pred71 <-knn(train = training, test = test, cl = train_label, k = 71, prob = TRUE)
pred71
CrossTable(x = test_label, y = pred71, prop.chisq = FALSE )
```

### ROC Curve for each k and display the AUC value:

```{r}
k <- c(1,5,11,21,51, 71)

par(mfrow=c(3,3))
for (i in k){
  pred <- knn(train = training, test = test, cl = train_label, k=i, prob=TRUE)
 
  prob <- attr(pred, "prob")
  prob1 <- ifelse(pred == "+", prob, 1-prob)
  
  res <- auc(test_label,prob1) 
  
  pred_knn <- ROCR::prediction(prob1, test_label)
  pred_knn <- performance(pred_knn, "tpr", "fpr")
  plot(pred_knn, avg= "threshold", colorize=T, lwd=3, 
       main=paste("ROC curve, k: ", i, ", auc=", round(res,4)))
}
```

(e) Commenting on the classification results based on the ROC curve, AUC value, and the number of false positives, false negatives, and classification error obtained for different values of k. The positive class assigned represents promoter sequences.
ROC Curves:
The points comprising the ROC curves indicate the true positive rate at different thresholds of false positives. To create the curves, the predictions of a classifier are sorted according to the estimated probability from the model for the positive class, with the highest values first.

AUC Value:
The closer the classifier's curve is to the perfect classifier, the better it is at identifying positive values. This can be measured using a statistic known as the Area Under the ROC Curve (AUC). The AUC treats the ROC diagram as a two-dimensional square and measures the total area under the ROC curve. AUC ranges from 0.5 (for a classifier with no predictive value) to 1.0 (for a perfect classifier). A convention for interpreting AUC scores uses a system similar to academic grades with letters:


‚Ä¢ A: Excellent = 0.9 to 1.0

‚Ä¢ B: Very Good = 0.8 to 0.9

‚Ä¢ C: Acceptable/Regular = 0.7 to 0.8

‚Ä¢ D: Poor = 0.6 to 0.7

‚Ä¢ F: No Discrimination = 0.5 to 0.6

As with most scales of this nature, the performance levels may work better for some tasks than others, and the categorization is somewhat subjective.

It is also worth noting that two ROC curves can have very different shapes yet have the same AUC. For this reason, AUC alone can be misleading. Best practice is to use AUC in combination with a qualitative examination of the ROC curve.

Source: [@lantz]

### ROC Curve for k=71:

The diagonal line from the bottom-left corner to the top-right corner of the diagram represents a classifier with no predictive value. This type of classifier detects true positives and false positives at the same rate, implying that the classifier cannot discriminate between them. This is the baseline against which other classifiers can be judged. ROC curves that fall close to this line indicate models that are not very useful. Additionally, it coincides with the lowest possible AUC value, 0.5.

### ROC Curves for k=51, k=21, k=5, k=11, k=1:

In this order, the curves and AUC values are represented from best classifier to worst classifier.

### Confusion Matrices:

From the values expressed in the Confusion Matrix, we can obtain a series of useful measures in our analysis.

Precision = (TP + TN) / (TP + FP + FN + TN)

Sensitivity = TP / (TP + FN)

Specificity = TN / (TN + FP)

```{r, include=FALSE}
conf1 <- confusionMatrix(table(test_label, pred1))
conf5 <- confusionMatrix(table(test_label, pred5))
conf11 <- confusionMatrix(table(test_label, pred11))
conf21 <- confusionMatrix(table(test_label, pred21))
conf51 <- confusionMatrix(table(test_label, pred51))
conf71 <- confusionMatrix(table(test_label, pred71))
```
```{r}
conf1
```

```{r}
conf5
```

```{r}
conf11
```

```{r}
conf21
```

```{r}
conf51
```

```{r}
conf71
```

For the value of k=1, we obtain False positive, FP=7 and False negative, FN=1. True positive, TP=16 and True negative, TN=11.

For the value of k=5, we obtain False positive, FP=6 and False negative, FN=2. True positive, TP=15 and True negative, TN=12.

For the value of k=11, we obtain False positive, FP=7 and False negative, FN=1. True positive, TP=16 and True negative, TN=11.

For the value of k=21, we obtain False positive, FP=9 and False negative, FN=1. True positive, TP=16 and True negative, TN=9.

For the value of k=51, we obtain False positive, FP=12 and False negative, FN=0. True positive, TP=17 and True negative, TN=6.

For the value of k=71, we obtain False positive, FP=18 and False negative, FN=0. True positive, TP=17 and True negative, TN=0.

### Classification error:

To calculate the classification error: (FP+FN)/(TP+TN+FP+FN)
From the values expressed in the Confusion Matrix, we can obtain a series of useful measures in our analysis.

```{r}
error_rate <- function(tp,tn,fp,fn){
  return((fp + fn)/(tp+tn+fp+fn))
}
# True Positive values for each value of k: 
tp1=16
tp5=15
tp11=16
tp21=16
tp51=17
tp71=17

# True Negative values for each value of k: 
tn1=11
tn5=12
tn11=11
tn21=9
tn51=6
tn71=0

# False Positive for each value of k: 
fp1=7
fp5=6
fp11=7
fp21=9
fp51=12
fp71=18

# False negative values for each value of k: 
fn1=1
fn5=2
fn11=1
fn21=1
fn51=0
fn71=0
  
error1 <- error_rate(tp = tp1, tn= tn1, fp = fp1, fn = fn1)
error5 <- error_rate(tp = tp5, tn= tn5, fp = fp5, fn = fn5)
error11 <- error_rate(tp = tp11, tn= tn11, fp = fp11, fn = fn11)
error21 <- error_rate(tp = tp21, tn= tn21, fp = fp21, fn = fn21)
error51 <- error_rate(tp = tp51, tn= tn51, fp = fp51, fn = fn51)
error71 <- error_rate(tp = tp71, tn= tn71, fp = fp71, fn = fn71)

print(paste0("error rate para k=1 = ", error1))
print(paste0("error rate para k=5 = ", error5))
print(paste0("error rate para k=11 = ", error11))
print(paste0("error rate para k=21 = ", error21))
print(paste0("error rate para k=51 = ", error51))
print(paste0("error rate para k=71 = ", error71))
```

The highest error rate is for k=71 as expected, followed by k=51, then k=21, and the rest have a similar classification error rate.

## 4. Representing sequence logos for each type of sequence (promoter/non-promoter). Comment on the results obtained.

We will use the toupper() function to convert the sequences to uppercase. Then we separate the promoter sequences from the non-promoter sequences and apply the ggseqlogo() function.

```{r}
datos <- read.table(file, header=FALSE, sep=",")

toupper <- toupper(datos$V3)
datos1 <- cbind.data.frame(datos$V1, toupper)

names(datos1)[1] = "V1"
head(datos1)

promotor <- datos1[datos1$V1 == "+",]
no_promotor <- datos1[datos1$V1 == "-",]

head(promotor)
head(no_promotor)
```

```{r}
ggseqlogo(promotor[,2])
```


```{r}
ggseqlogo(no_promotor[,2])
```

Representing sequence logos allows us to observe the degree of conservation in a sequence.

ggseqlogo() shows us the probabilities of nucleotide occurrence at each position in the sequence. The nucleotides that appear at the top of the graph and with larger size are the most probable ones at each position.

The variability of the sequences is represented by the size of the letters, so the fewer different nucleotides present, the larger the letter, indicating better conservation and higher frequency or probability.

In the promoter sequence graph, three positions with significantly less variability can be clearly observed compared to all others. The larger letters in these positions are T, T, and G, indicating that these nucleotides predominate in these positions. The frequencies are significantly higher than those in the non-promoter sequence graph, specifically 10 times higher.

However, the non-promoter sequence graph exhibits positions with varying variability, with one position standing out as the best conserved. The predominant nucleotide in this position is G.

